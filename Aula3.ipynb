{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/detauportuno/Projeto_Pinturas_PAE/blob/main/Aula3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhUrfA7OE7_U"
      },
      "source": [
        "#Introdução à inteligência artificial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFga6z6UDzaU"
      },
      "source": [
        "## Aula 3 - Introdução ao PyTorch\n",
        "\n",
        "Neste caderno, você será apresentado ao [PyTorch](http://pytorch.org/), uma estrutura para construir e treinar redes neurais. O PyTorch se comporta de várias maneiras como as matrizes que você ama do Numpy. Afinal, essas matrizes Numpy são apenas tensores. O PyTorch pega esses tensores e facilita a sua transferência para GPUs para o processamento mais rápido necessário ao treinar redes neurais. Ele também fornece um módulo que calcula automaticamente gradientes (para retropropagação!) E outro módulo especificamente para a construção de redes neurais. No conjunto, o PyTorch acaba sendo mais coerente com o Python e a pilha Numpy/Scipy em comparação com o TensorFlow e outras estruturas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EteQisTVEZCz"
      },
      "source": [
        "### Tensores\n",
        "\n",
        "Acontece que os cálculos de redes neurais são apenas um monte de operações de álgebra linear em *tensores*, uma generalização de matrizes. Um vetor é um tensor unidimensional, uma matriz é um tensor bidimensional, uma matriz com três índices é um tensor tridimensional (imagens em cores RGB, por exemplo). A estrutura de dados fundamental para redes neurais são os tensores e o PyTorch (assim como praticamente qualquer outra estrutura de aprendizado profundo) é construído em torno dos tensores.\n",
        "\n",
        "Com o básico abordado, é hora de explorar como podemos usar o PyTorch para construir uma rede neural simples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WuGqLm-KEYfY"
      },
      "source": [
        "# Vamos importar o PyTorch\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkrI4EenGAJ8"
      },
      "source": [
        "def activation(x):\n",
        "    \"\"\" Função de ativação sigmóide\n",
        "\n",
        "        Argumentos\n",
        "        ---------\n",
        "        x: torch.Tensor\n",
        "    \"\"\"\n",
        "    return 1/(1+torch.exp(-x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWFR7ih9F3HQ"
      },
      "source": [
        "### Gere alguns dados\n",
        "torch.manual_seed(7) # Defina a semente aleatória para que as coisas sejam previsíveis\n",
        "\n",
        "# Recursos são 5 variáveis normais aleatórias\n",
        "features = torch.randn((1, 5))\n",
        "# Pesos verdadeiros para nossos dados, variáveis normais aleatórias novamente\n",
        "weights = torch.randn_like(features)\n",
        "# e um termo verdadeiro viés\n",
        "bias = torch.randn((1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKVXG2VUGWIP"
      },
      "source": [
        "Acima, geramos dados que podemos usar para obter a saída de nossa rede simples. Por enquanto, tudo é aleatório. No futuro, começaremos a usar dados normais. Passando por cada linha relevante:\n",
        "\n",
        "`features = torch.randn((1, 5))` cria um tensor com a forma `(1, 5)`, uma linha e cinco colunas, que contém valores distribuídos aleatoriamente de acordo com a distribuição normal, com média de zero e padrão desvio de um.\n",
        "\n",
        "`weights = torch.randn_like(features)` cria outro tensor com a mesma forma que `features`, novamente contendo valores de uma distribuição normal.\n",
        "\n",
        "Finalmente, `bias = torch.randn((1, 1))` cria um valor único a partir de uma distribuição normal.\n",
        "\n",
        "Os tensores PyTorch podem ser adicionados, multiplicados, subtraídos, etc., assim como os arrays Numpy. Em geral, você usará os tensores PyTorch da mesma maneira que usaria as matrizes Numpy. Eles trazem alguns benefícios interessantes, como a aceleração da GPU, que veremos mais adiante. Por enquanto, use os dados gerados para calcular a saída dessa rede simples de camada única.\n",
        "> **Exemplo**: Calcule a saída da rede com recursos de entrada `features`, pesos `weights` e viés `bias`. Semelhante ao Numpy, o PyTorch possui a função [`torch.sum()`](https://pytorch.org/docs/stable/torch.html#torch.sum), além do método `.sum()` em tensores, para somar os resultados. Use a função 'activation' definida acima como a função de ativação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKx8H_fQG_zR"
      },
      "source": [
        "output = activation(torch.sum(weights*features)+bias)\n",
        "output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxwze_16IZ8W"
      },
      "source": [
        "Você pode fazer a multiplicação e somar na mesma operação usando uma multiplicação de matrizes. Em geral, convém usar multiplicações de matriz, pois elas são mais eficientes e aceleradas usando bibliotecas modernas e computação de alto desempenho em GPUs.\n",
        "\n",
        "Aqui, queremos fazer uma multiplicação matricial dos recursos e pesos. Para isso, podemos usar [`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm) ou [`torch.matmul()`](https://pytorch.org/docs/stable/torch.html#torch.matmul), que é um pouco mais complicado e suporta transmissão. Se tentarmos fazê-lo com os 'features' e 'weights' como eles são, obteremos um erro:\n",
        "\n",
        "```python\n",
        ">> torch.mm(features, weights)\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "RuntimeError                              Traceback (most recent call last)\n",
        "<ipython-input-13-15d592eb5279> in <module>()\n",
        "----> 1 torch.mm(features, weights)\n",
        "\n",
        "RuntimeError: size mismatch, m1: [1 x 5], m2: [1 x 5] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:2033\n",
        "```\n",
        "\n",
        "Ao criar redes neurais em qualquer estrutura, você verá isso com frequência. Realmente frequentemente. O que está acontecendo aqui é que nossos tensores não têm as formas corretas para realizar uma multiplicação de matrizes. Lembre-se de que para multiplicações de matrizes, o número de colunas no primeiro tensor deve ser igual ao número de linhas na segunda coluna. Ambos os `features` e `weights` têm a mesma forma, `(1, 5)`. Isso significa que precisamos alterar a forma dos pesos para que a multiplicação da matriz funcione.\n",
        "\n",
        "**Nota:** Para ver a forma de um tensor chamado `tensor`, use `tensor.shape`. Se você estiver construindo redes neurais, estará usando esse método frequentemente.\n",
        "\n",
        "Existem algumas opções aqui: [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`weights.resize_()`]( https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_) e [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).\n",
        "\n",
        "* `weights.reshape(a, b)` retornará um novo tensor com os mesmos dados que `weights` com tamanho `(a, b)` às vezes e às vezes um clone, pois ele copia os dados para outra parte do memória.\n",
        "* `weights.resize_(a, b)` retorna o mesmo tensor com uma forma diferente. No entanto, se a nova forma resultar em menos elementos que o tensor original, alguns elementos serão removidos do tensor (mas não da memória). Se a nova forma resultar em mais elementos que o tensor original, novos elementos serão não inicializados na memória. Aqui, devo observar que o sublinhado no final do método indica que esse método é realizado ** no local **. Aqui está um ótimo tópico do fórum para [leia mais sobre operações locais](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) no PyTorch.\n",
        "* `weights.view(a, b)` retornará um novo tensor com os mesmos dados que `weights` com tamanho` (a, b) `.\n",
        "\n",
        "Portanto, agora podemos remodelar 'weights' para ter cinco linhas e uma coluna com algo como 'weights.view(5, 1)'.\n",
        "\n",
        "> **Exemplo**: Calcule a saída da nossa pequena rede usando multiplicação de matrizes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6ZzGykeIZaV"
      },
      "source": [
        "output = activation(torch.mm(features,weights.view(5, 1))+bias)\n",
        "output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0FPwmnUK89N"
      },
      "source": [
        "### Junte tudo\n",
        "É assim que você pode calcular a saída para um único neurônio. O poder real desse algoritmo acontece quando você começa a empilhar essas unidades individuais em camadas e pilhas de camadas, em uma rede de neurônios. A saída de uma camada de neurônios se torna a entrada para a próxima camada. Com várias unidades de entrada e unidades de saída, agora precisamos expressar os pesos como uma matriz.\n",
        "\n",
        "A primeira camada mostrada na parte inferior aqui são as entradas, compreensivelmente chamadas de **camada de entrada**. A camada do meio é chamada de **camada oculta** e a camada final (à direita) é a **camada de saída**. Podemos expressar essa rede matematicamente com matrizes novamente e usar a multiplicação de matrizes para obter combinações lineares para cada unidade em uma operação. Por exemplo, a camada oculta ($ h_1 $ e $ h_2 $ aqui) pode ser\n",
        "\n",
        "$$\n",
        "\\vec{h} = [h_1 \\, h_2] =\n",
        "\\begin{bmatrix}\n",
        "x_1 \\, x_2 \\cdots \\, x_n\n",
        "\\end{bmatrix}\n",
        "\\cdot\n",
        "\\begin{bmatrix}\n",
        "           w_{11} & w_{12} \\\\\n",
        "           w_{21} &w_{22} \\\\\n",
        "           \\vdots &\\vdots \\\\\n",
        "           w_{n1} &w_{n2}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "A saída para esta pequena rede é encontrada tratando a camada oculta como entradas para a unidade de saída. A saída da rede é expressa simplesmente\n",
        "\n",
        "$$\n",
        "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N8zkDLTKwps"
      },
      "source": [
        "torch.manual_seed(7)\n",
        "\n",
        "# Os 'features' são três variáveis normais aleatórias\n",
        "features = torch.randn((1, 3))\n",
        "\n",
        "# Defina o tamanho de cada camada em nossa rede\n",
        "n_input = features.shape[1]     # Número de unidades de entrada, deve corresponder ao número de 'features' de entrada\n",
        "n_hidden = 2                    # Número de unidades ocultas\n",
        "n_output = 1                    # Número de unidades de saída\n",
        "\n",
        "# 'Weights' para entradas na camada oculta\n",
        "W1 = torch.randn(n_input, n_hidden)\n",
        "# 'Weights' da camada oculta para a camada de saída\n",
        "W2 = torch.randn(n_hidden, n_output)\n",
        "\n",
        "# e 'bias' para camadas ocultas e de saída\n",
        "B1 = torch.randn((1, n_hidden))\n",
        "B2 = torch.randn((1, n_output))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltg_VslxLsME"
      },
      "source": [
        "> **Exercício:** Calcule a saída desta rede de múltiplas camadas usando os 'weights' `W1` e `W2`, e os 'bias', `B1` e `B2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxpPaVoCLuAe"
      },
      "source": [
        "h = None\n",
        "print(h)\n",
        "output = None\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpB-ZYELMH6V"
      },
      "source": [
        "Se você fez isso corretamente, deverá ver a saída da rede igual a `tensor([[0,3171]]) '.\n",
        "\n",
        "O número de unidades ocultas é um parâmetro da rede, geralmente chamado de **hiperparâmetro** para diferenciá-lo dos parâmetros de pesos e desvios. Como você verá mais adiante, quando discutirmos o treinamento de uma rede neural, quanto mais unidades ocultas uma rede tiver e mais camadas, maior será a capacidade de aprender com os dados e fazer previsões precisas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b011dlplPvXu"
      },
      "source": [
        "\n",
        "## Redes neurais com PyTorch\n",
        "\n",
        "As redes de aprendizado profundo tendem a ser massivas com dezenas ou centenas de camadas, é daí que o termo \"profundo\" vem. Você pode construir uma dessas redes profundas usando apenas matrizes de peso, como fizemos no notebook anterior, mas, em geral, é muito complicado e difícil de implementar. O PyTorch possui um ótimo módulo `nn`, que fornece uma boa maneira de construir eficientemente grandes redes neurais."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m7QvgG8L-CW"
      },
      "source": [
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-GE79npQQ4Z"
      },
      "source": [
        "Agora vamos construir uma rede maior que possa resolver um problema (anteriormente) difícil, identificando texto em uma imagem. Aqui, usaremos o conjunto de dados MNIST que consiste em dígitos manuscritos em escala de cinza. Cada imagem tem 28x28 pixels. Você pode ver uma amostra abaixo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDBRP-ggPdV5"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Defina uma transformação para normalizar os dados\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.5,), (0.5,)),\n",
        "                              ])\n",
        "\n",
        "# Faça o download e carregue os dados de treinamento\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8HWrtZySqj5"
      },
      "source": [
        "Você notará que foi criado o `trainloader` com um tamanho de lote de 64 e `shuffle = True`. O tamanho do lote é o número de imagens que obtemos em uma iteração do carregador de dados e passamos por nossa rede, geralmente chamada de *batch*. E `shuffle = True` diz para ele embaralhar o conjunto de dados toda vez que começamos a percorrer o carregador de dados novamente. Mas aqui estou apenas pegando o primeiro lote para que possamos verificar os dados. Podemos ver abaixo que `images` é apenas um tensor com tamanho `(64, 1, 28, 28) `. Portanto, 64 imagens por lote, 1 canal de cor e imagens 28x28."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFFSavQYPkQe"
      },
      "source": [
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "print(type(images)) # Tipo da informação\n",
        "print(images.shape) # Quantidade de imagens e tamanho de cada imagem\n",
        "print(labels.shape) # Quantidade de 'labels'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1ya0L2MPdyv"
      },
      "source": [
        "# Mostrar imagem aleatória\n",
        "plt.imshow(images[2].numpy().squeeze(), cmap='Greys_r');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwgIrW7xTHMM"
      },
      "source": [
        "Primeiro, vamos tentar construir uma rede simples para esse conjunto de dados usando matrizes de peso e multiplicações de matrizes. Em seguida, veremos como fazer isso usando o módulo `nn` do PyTorch, que fornece um método muito mais conveniente e poderoso para definir arquiteturas de rede.\n",
        "\n",
        "As redes que você viu até agora são chamadas de redes *totalmente conectadas* ou *densas*. Cada unidade em uma camada é conectada a cada unidade na próxima camada. Em redes totalmente conectadas, a entrada para cada camada deve ser um vetor unidimensional (que pode ser empilhado em um tensor 2D como um lote de vários exemplos). No entanto, nossas imagens são tensores 2D de 28x28, portanto, precisamos convertê-los em vetores 1D. Pensando nos tamanhos, precisamos converter o lote de imagens com a forma `(64, 1, 28, 28)` para uma forma de `(64, 784)`, 784 é 28 vezes 28. Isso é normalmente chamado de *achatamento*, achatamos as imagens 2D em vetores 1D.\n",
        "\n",
        "Anteriormente, você construiu uma rede com uma unidade de saída. Aqui precisamos de 10 unidades de saída, uma para cada dígito. Queremos que nossa rede preveja o dígito mostrado em uma imagem; portanto, o que faremos é calcular as probabilidades de que a imagem seja de qualquer dígito ou classe. Isso acaba sendo uma distribuição de probabilidade discreta nas classes (dígitos) que nos diz a classe mais provável para a imagem. Isso significa que precisamos de 10 unidades de saída para as 10 classes (dígitos). Veremos como converter a saída da rede em uma distribuição de probabilidade a seguir.\n",
        "\n",
        "> **Exercício:** Achatar o lote de imagens `images`. Em seguida, construa uma rede de várias camadas com 784 unidades de entrada, 256 unidades ocultas e 10 unidades de saída usando tensores aleatórios para os pesos e desvios. Por enquanto, use uma ativação sigmóide para a camada oculta. Deixe a camada de saída sem uma ativação; adicionaremos uma que nos fornecerá uma distribuição de probabilidade a seguir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Gk5HowPPmHU"
      },
      "source": [
        "def activation(x):\n",
        "    \"\"\" Função de ativação sigmóide\n",
        "\n",
        "        Argumentos\n",
        "        ---------\n",
        "        x: torch.Tensor\n",
        "    \"\"\"\n",
        "    return 1/(1+torch.exp(-x))\n",
        "\n",
        "imgVectorized = images.view(64,-1) # Faça o \"achatamento\" do lote de imagem\n",
        "\n",
        "\n",
        "n_input = imgVectorized.shape[1]     # Número de unidades de entrada, deve corresponder ao número de 'features' de entrada\n",
        "n_hidden = 256                    # Número de unidades ocultas\n",
        "n_output = 10                    # Número de unidades de saída\n",
        "\n",
        "# Weights\n",
        "W1 = torch.randn(n_input, n_hidden)\n",
        "W2 = torch.randn(n_hidden, n_output)\n",
        "\n",
        "# Bias\n",
        "B1 = torch.randn((1, n_hidden))\n",
        "B2 = torch.randn((1, n_output))\n",
        "\n",
        "h = activation(torch.mm(imgVectorized,W1)+B1)\n",
        "out = activation(torch.mm(h,W2)+B2)\n",
        "\n",
        "out.size()\n",
        "# O resultado deve ser 'torch.Size([64, 10])'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qITv1WEdWVp7"
      },
      "source": [
        "Agora temos 10 saídas para nossa rede. Queremos passar uma imagem para a nossa rede e obter uma distribuição de probabilidade nas classes que nos diz as classes prováveis às quais a imagem pertence. Algo que se parece com isso:\n",
        "\n",
        "Aqui vemos que a probabilidade para cada classe é aproximadamente a mesma. Isso representa uma rede não treinada, mas ainda não viu nenhum dado, apenas retorna uma distribuição uniforme com probabilidades iguais para cada classe.\n",
        "\n",
        "Para calcular essa distribuição de probabilidade, geralmente usamos a função [**softmax**](https://en.wikipedia.org/wiki/Softmax_function). Matematicamente isso parece:\n",
        "\n",
        "\n",
        "$$\n",
        "\\Large \\sigma(x_i) = \\cfrac{e^{x_i}}{\\sum_k^K{e^{x_k}}}\n",
        "$$\n",
        "\n",
        "\n",
        "O que isso faz é espremer cada entrada $ x_i $ entre 0 e 1 e normaliza os valores para fornecer uma distribuição de probabilidade adequada, na qual as probabilidades atingem um.\n",
        "\n",
        "> **Exercício:** Implemente uma função `softmax` que execute o cálculo do softmax e retorne as distribuições de probabilidade para cada exemplo no lote. Observe que você precisará prestar atenção às formas ao fazer isso. Se você tiver um tensor `a` com forma `(64, 10)` e um tensor `b` com forma `(64,)`, fazer `a / b` gerará um erro porque o PyTorch tentará fazer o divisão nas colunas (chamada de transmissão), mas você encontrará uma incompatibilidade de tamanho. A maneira de pensar sobre isso é para cada um dos 64 exemplos, você só deseja dividir por um valor, a soma no denominador. Então você precisa que `b` tenha a forma de` (64, 1) `. Dessa forma, o PyTorch dividirá os 10 valores em cada linha de `a` pelo valor único em cada linha de` b`. Preste atenção em como você recebe a soma também. Você precisará definir a palavra-chave `dim` em` torch.sum`. Definir `dim=0` leva a soma entre as linhas enquanto  `dim=1` leva a soma entre as colunas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UqfbYmAWSz6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "57614675-a226-4373-f4d8-1582885df692"
      },
      "source": [
        "def softmax(x):\n",
        "    ## TODO: Implementar softmax aqui\n",
        "    exp_tensor = torch.exp(x)\n",
        "    sum_exp_tensor = torch.sum(torch.exp(x), dim=) # As somas devem ser entre colunas\n",
        "    return exp_tensor/sum_exp_tensor.view(-1, 1)\n",
        "\n",
        "probabilities = softmax(out)\n",
        "\n",
        "# Tem a forma correta? Deve ser (64, 10)\n",
        "print(probabilities.shape)\n",
        "# A soma equivale a 1?\n",
        "print(probabilities.sum(dim=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-2d3b5a4ede01>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    sum_exp_tensor = torch.sum(torch.exp(x), dim=) # As somas devem ser entre colunas\u001b[0m\n\u001b[0m                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s4MrLJAxZEt"
      },
      "source": [
        "Até agora, apenas observamos a ativação do softmax, mas em geral qualquer função pode ser usada como uma função de ativação. O único requisito é que, para que uma rede se aproxime de uma função não linear, as funções de ativação devem ser não lineares. Aqui estão mais alguns exemplos de funções de ativação comuns: Tanh (tangente hiperbólica) e ReLU (unidade linear retificada).\n",
        "\n",
        "![activations](https://www.kdnuggets.com/wp-content/uploads/activation.png)\n",
        "\n",
        "Na prática, a função ReLU é usada quase exclusivamente como a função de ativação para camadas ocultas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVzHocG0ylO0"
      },
      "source": [
        "### Criando a rede neural\n",
        "\n",
        "![neural](https://raw.githubusercontent.com/dmlc/web-data/master/mxnet/image/mlp_mnist.png)\n",
        "\n",
        "> **Exercício:** Crie uma rede com 784 unidades de entrada, uma camada oculta com 128 unidades e uma ativação ReLU, depois uma camada oculta com 64 unidades e uma ativação ReLU e, finalmente, uma camada de saída com uma ativação softmax como mostrado acima . Você pode usar uma ativação ReLU com o módulo `nn.ReLU` ou a função` F.relu`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQdpEvF_zzc5"
      },
      "source": [
        "def view_classify(img, ps, version=\"MNIST\"):\n",
        "    ps = ps.data.numpy().squeeze()\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
        "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
        "    ax1.axis('off')\n",
        "    ax2.barh(np.arange(10), ps)\n",
        "    ax2.set_aspect(0.1)\n",
        "    ax2.set_yticks(np.arange(10))\n",
        "    if version == \"MNIST\":\n",
        "        ax2.set_yticklabels(np.arange(10))\n",
        "    ax2.set_title('Probabilidade de classe')\n",
        "    ax2.set_xlim(0, 1.1)\n",
        "\n",
        "    plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOM5qdJZViXZ"
      },
      "source": [
        "# Hiperparâmetros para nossa rede\n",
        "input_size = None\n",
        "hidden_sizes = [None, None] #vetor com duas posições\n",
        "output_size = None\n",
        "\n",
        "# Construa uma rede de feed-forward\n",
        "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(hidden_sizes[1], output_size))\n",
        "print(model)\n",
        "\n",
        "# Passagem direta pela rede e mostra imagem\n",
        "images, labels = next(iter(trainloader))\n",
        "images.resize_(images.shape[0], 1, 784)\n",
        "m = nn.Softmax(dim=1)\n",
        "ps = m(model.forward(images[0,:]))\n",
        "view_classify(images[0].view(1, 28, 28), ps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rQEsJ_Fcsz1",
        "cellView": "form"
      },
      "source": [
        "#@title Treine a rede\n",
        "from torch import optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "epochs = 5\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "        images = images.view(images.shape[0], -1)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(f\"Perda de treinamento: {running_loss/len(trainloader)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv7lrk6Cmm51",
        "cellView": "form"
      },
      "source": [
        "#@title Execute para ver o resultado\n",
        "images, labels = next(iter(trainloader))\n",
        "img = images[1].view(1, 784)\n",
        "with torch.no_grad():\n",
        "    logps = model(img)\n",
        "m = nn.Softmax(dim=1)\n",
        "ps = m(logps)\n",
        "view_classify(img.view(1, 28, 28), ps)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}